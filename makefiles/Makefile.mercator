################################################################################
#   Gene prediction pipeline 
#
#   $Id: Makefile.mercator 698 2006-07-19 15:53:22Z andreas $
#
#   Copyright (C) 2004 Andreas Heger
#
#   This program is free software; you can redistribute it and/or
#   modify it under the terms of the GNU General Public License
#   as published by the Free Software Foundation; either version 2
#   of the License, or (at your option) any later version.
#
#   This program is distributed in the hope that it will be useful,
#   but WITHOUT ANY WARRANTY; without even the implied warranty of
#   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#   GNU General Public License for more details.
#
#   You should have received a copy of the GNU General Public License
#   along with this program; if not, write to the Free Software
#   Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.
#################################################################################
SHELL=/bin/bash --login

LOG=log

################################################
## Section parameters: start
################################################
## project name
PARAM_PROJECT_NAME?=geneprediction
################################################
## directories
## directory where gene prediction scripts can be found
DIR_SCRIPTS_GENEPREDICTION?=/home/andreas/gpipe/
## directory where various helper scripts can be found
DIR_SCRIPTS_TOOLS?=/net/cpp-group/scripts/tools/
## shared directory, path for submit host
DIR_TMP_SHARED_LOCAL?=/net/cpp-group/gpipe/tmp/$(PARAM_PROJECT_NAME)/
## shared directory, path for cluster node
DIR_TMP_SHARED_REMOTE?=/net/cpp-group/gpipe/tmp/$(PARAM_PROJECT_NAME)/

################################################
## Cluster parameters
## queue to submit to
PARAM_QUEUE?=bc1.q,bc2.q
## command to use to copy between shared and remote directory
PARAM_CMD_COPY?=ln -f -s $(CURDIR)/

## list of genomes
PARAM_GENOMES=dana dere dyak dpse dvir dmoj dmel

#########################################################################
## pattern for genome and peptide files. If there is a single file, set to filename
PARAM_INPUT_GENOME?=genome_%s.fasta
PARAM_INPUT_PEPTIDES?=peptides.fasta
PARAM_INPUT_GENOME?=$(wildcard genome*.fasta)
PARAM_INPUT_EXONS?=reference.exons


################################################
# Section parameters: end
################################################



prepare_chroms:
	grep -v "#" ../../../D_pseudoobscura/contig_sizes | cut -f 1,2 > dpse.chroms
	grep -v "#" ../../../D_melanogaster/contig_sizes | cut -f 1,2 > dmel.chroms
	grep -v "#" ../../../D_ananassae/contig_sizes | cut -f 1,2 > dana.chroms
	grep -v "#" ../../../D_virilis/contig_sizes | cut -f 1,2 > dvir.chroms
	grep -v "#" ../../../D_mojavensis/contig_sizes | cut -f 1,2 > dmoj.chroms
	grep -v "#" ../../../D_erecta/contig_sizes | cut -f 1,2 > dere.chroms
	grep -v "#" ../../../D_yakuba/contig_sizes | cut -f 1,2 > dyak.chroms

prepare_anchorsx:
	grep -v "#" ../../../D_pseudoobscura/export_cds.gff |\
	awk '{printf("%s\t%s\t%s\t%s\t%s\n", $$10,$$1,$$7,$$4,$$5);}' > dpse.anchors


prepare_anchors:
	for x in $(PARAM_GENOMES); do \
		perl -p -e "s/^dmel/pdmel/" ../../exon_boundaries | grep "^p$$x" |\
		awk '{printf("%s:%s\t%s\t%s\t%s\t%s\n", $$1, $$5, $$2, $$3, $$8, $$9);}' > $${x}.anchors; \
	done

prepare_links:
	python $(DIR_SCRIPTS_GENEPREDICTION)optic/links2exons.py \
	--cds=../../exon_boundaries --map=peptides2cds < ../../blast_asymmetric.links > all.links

prepare.links2:
	for x in $(PARAM_GENOMES); do \
		for y in $(PARAM_GENOMES); do \
			if test ! -e $${y}-$${x}.hits && [ $$x != $$y ]; then \
				echo "p$$x p$$y"; \
				cat all.links | perl -p -e 's/([^_]+)dmel:/$$1pdmel:/' |\
				grep "p$$x" | grep "p$$y" |\
				awk -v x=$$x -v y=$$y '{if (match($$1, x) && match($$3, y) ) \
					{ printf("%s:%s\t%s:%s\t%i\t%s\n", $$1, $$2, $$3, $$4, -log($$5), $$5);} \
				      else \
					{ printf("%s:%s\t%s:%s\t%i\t%s\n", $$3, $$4, $$1, $$2, -log($$5), $$5);} \
		 		     }' |\
				sort | uniq > $${x}-$${y}.hits; \
			fi \
		done \
	done
